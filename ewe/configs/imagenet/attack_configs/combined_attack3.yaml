attack_list: [attack1, attack2]

create:
  name: stacked_attack

remove:
  name: stacked_attack_removal

dataset:
  name: ImageNetDataLoader
  subset: attacker
  root: ~/.wrt/data
  batch_size: 32
  image_size: 224
  num_workers: 16
  n_train: 50000
  n_test: 1000

# This dataset is not used. True labels are only given to speed up runtime.
true_labels: False
name: combined_attack3
output_dir: outputs/imagenet/attacks/combined_attack3/

attack1:
  pretrained:
    jia: outputs/imagenet/attacks/cross_architecture_retraining/00000_retraining_attack_wm_jia/checkpoint.pth

  create:
    name: retraining_attack
    num_classes: 1000
    image_size: 224

  remove:
    name: retraining_removal
    epochs: 60
    batch_size: 64

  surrogate_model:
    name: ImageNetDenseNetModel
    num_classes: 1000
    image_size: 224

  dataset:
    name: ImageNetDataLoader
    subset: all
    root: ~/.wrt/data
    batch_size: 32
    image_size: 224
    num_workers: 16

  optimizer:
    name: SGD
    lr: 0.1
    weight_decay: 0.0001
    momentum: 0.9

  scheduler:
    name: ReduceLROnPlateau
    factor: 0.1
    patience: 2
    verbose: True

  true_labels: False
  name: cross_architecture_retraining_attack
  output_dir: outputs/imagenet/attacks/cross_architecture_retraining/

attack2:
  create:
    name: adversarial_training_attack
    eps: [ 0.01, 0.1, 0.25 ]
    method: fgm
    eps_step: 0.01
    max_iter: 30
    num_classes: 1000

  remove:
    name: adversarial_training_removal
    epochs: 3
    target_label: 4
    boost_factor: 3              # Number of times each sample is repeated per epoch.
    n_max: 5000                 # Maximal number of adversarial examples to generate per epoch.
    check_every_n_batches: 200   # Evaluate the watermark accuracy every 200 batches.

  dataset:
    name: ImageNetDataLoader
    subset: attacker
    root: ~/.wrt/data
    apply_augmentation: False
    batch_size: 32
    image_size: 224
    num_workers: 16
    n_train: 100000

  true_labels: False
  name: adversarial_training
  output_dir: outputs/imagenet/attacks/adversarial_training/





